{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from firecloud import fiss\n",
    "from google.cloud import storage\n",
    "import firecloud.api as fapi\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Scientific computing in python\n",
    "import numpy as np\n",
    "# Data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# Data analysis tools and data structures like the DataFrame\n",
    "import pandas as pd\n",
    "# Statistical data visualization, site: https://seaborn.pydata.org/\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the Google billing project name and workspace name\n",
    "project = os.environ['WORKSPACE_NAMESPACE']\n",
    "workspace = os.environ['WORKSPACE_NAME']\n",
    "bucket = os.environ['WORKSPACE_BUCKET'] + \"/\"\n",
    "google_project = os.environ['GOOGLE_PROJECT']\n",
    "\n",
    "# Verify that we've captured the environment variables\n",
    "print(\"Terra Billing project: \" + project)\n",
    "print(\"Workspace: \" + workspace)\n",
    "print(\"Workspace storage bucket: \" + bucket)\n",
    "print(\"Google project: \" + google_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def list_gcs_files(bucket_name, prefix):\n",
    "    # Initialize a client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # List blobs in the specified directory\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "\n",
    "# Replace with your bucket name and directory prefix\n",
    "bucket_name = 'fc-xxxxxx-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n",
    "prefix = 'working-set-redeposit'\n",
    "\n",
    "list_gcs_files(bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Hail\n",
    "hl.init(default_reference=\"GRCh38\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Define 163k WGS paths ------------------- #\n",
    "wgs_paths = {\n",
    "    1: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_1\",\n",
    "    2: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_2\",\n",
    "    3: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_3\",\n",
    "    4: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_4\",\n",
    "    5: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_5\",\n",
    "    6: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_6\",\n",
    "    7: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_7\",\n",
    "    8: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_8\",\n",
    "    9: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_9\",\n",
    "    10: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_10\",\n",
    "    11: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_11\",\n",
    "    12: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_12\",\n",
    "    13: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_13\",\n",
    "    14: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_14\",\n",
    "    15: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_15\",\n",
    "    16: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_16\",\n",
    "    17: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_17\",\n",
    "    18: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_18\",\n",
    "    19: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_19\",\n",
    "    20: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_20\",\n",
    "    21: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_21\",\n",
    "    22: \"gs://fc-secure/call-prune_for_relatedness_testing/agd163k_22\"\n",
    "}\n",
    "\n",
    "# ------------------- Function to Load & Filter PLINK Data ------------------- #\n",
    "def load_and_filter_plink(paths_dict):\n",
    "    mt_list = []\n",
    "    for chrom, plink_prefix in paths_dict.items():\n",
    "        print(f\"Loading chromosome {chrom} from {plink_prefix}\")\n",
    "        mt = hl.import_plink(\n",
    "            bed = plink_prefix + \".bed\",\n",
    "            bim = plink_prefix + \".bim\",\n",
    "            fam = plink_prefix + \".fam\",\n",
    "            reference_genome = \"GRCh38\"\n",
    "        )\n",
    "        # Filter rows by joining with snp_keep_table via rsid\n",
    "        #mt_filtered = mt.filter_rows(hl.is_defined(snp_keep_table[mt.rsid]))\n",
    "        mt_list.append(mt)\n",
    "    # Iteratively union the rows from each chromosome\n",
    "    combined = mt_list[0]\n",
    "    for mt in mt_list[1:]:\n",
    "        combined = combined.union_rows(mt)\n",
    "    return combined\n",
    "\n",
    "print(\"Loading WGS dataset...\")\n",
    "mt_wgs = load_and_filter_plink(wgs_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exporting as PLINK...\")\n",
    "hl.export_plink(mt_wgs, \"gs://fc-secure/relatedness_testing/subset_163k/163k_pruned_missingness0.999_biallelic_maf0.3_ld_50000_10_0.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "#In case additional filtering is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from firecloud import fiss\n",
    "from google.cloud import storage\n",
    "import firecloud.api as fapi\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "\n",
    "# Scientific computing in python\n",
    "import numpy as np\n",
    "# Data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "# Data analysis tools and data structures like the DataFrame\n",
    "import pandas as pd\n",
    "# Statistical data visualization, site: https://seaborn.pydata.org/\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the Google billing project name and workspace name\n",
    "project = os.environ['WORKSPACE_NAMESPACE']\n",
    "workspace = os.environ['WORKSPACE_NAME']\n",
    "bucket = os.environ['WORKSPACE_BUCKET'] + \"/\"\n",
    "google_project = os.environ['GOOGLE_PROJECT']\n",
    "\n",
    "# Verify that we've captured the environment variables\n",
    "print(\"Terra Billing project: \" + project)\n",
    "print(\"Workspace: \" + workspace)\n",
    "print(\"Workspace storage bucket: \" + bucket)\n",
    "print(\"Google project: \" + google_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def list_gcs_files(bucket_name, prefix):\n",
    "    # Initialize a client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # List blobs in the specified directory\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "\n",
    "# Replace with your bucket name and directory prefix\n",
    "bucket_name = 'fc-secure'\n",
    "prefix = 'relatedness_testing/subset_163k'\n",
    "\n",
    "list_gcs_files(bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(bucket_name, destination_folder_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_folder_name)\n",
    "\n",
    "    blob.upload_from_string('')\n",
    "\n",
    "    print('Created {} .'.format(\n",
    "        destination_folder_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "hl.init(default_reference='GRCh38', idempotent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.import_plink(\n",
    "bed='gs://fc-secure/relatedness_testing/subset_163k/163k_pruned_missingness0.999_biallelic_maf0.3_ld_50000_10_0.8.bed',\n",
    "bim='gs://fc-secure/relatedness_testing/subset_163k/163k_pruned_missingness0.999_biallelic_maf0.3_ld_50000_10_0.8.bim',\n",
    "fam='gs://fc-secure/relatedness_testing/subset_163k/163k_pruned_missingness0.999_biallelic_maf0.3_ld_50000_10_0.8.fam',\n",
    "reference_genome='GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute variant QC metrics\n",
    "mt = hl.variant_qc(mt)\n",
    "\n",
    "# Filter SNPs: call rate â‰¥ 0.9999999\n",
    "mt = mt.filter_rows((mt.variant_qc.call_rate >= 0.999999999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of variants after pruning: {mt.count_rows()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exporting as PLINK...\")\n",
    "hl.export_plink(mt, \"gs://fc-secure/relatedness_testing/subset_163k/163k_pruned_missingness0.999_biallelic_maf0.3_ld_50000_10_0.8_50k_snps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter to include only biallelic variants\n",
    "# biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2)\n",
    "\n",
    "# # Now perform LD pruning on the biallelic dataset\n",
    "# pruned_variant_table = hl.ld_prune(biallelic_mt.GT, r2=0.8, bp_window_size=20000)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
